{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from gym import spaces\n",
    "from gym import envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#behavior policy for on policy control\n",
    "def epsilon_greedy_policy(s, nA, q, epsilon):\n",
    "    A = np.ones(nA, dtype = float) * epsilon / nA\n",
    "    best_action = np.argmax(q[s])\n",
    "    A[best_action] += (1.0 - epsilon)\n",
    "    return A\n",
    "\n",
    "#behavior policy for off policy control\n",
    "def random_policy(s, nA):\n",
    "    A = np.ones(nA, dtype=float) / nA\n",
    "    return A\n",
    "\n",
    "#target policy for off policy control\n",
    "def greedy_policy(s, nA, q):\n",
    "    A = np.zeros(nA, dtype = float)\n",
    "    best_action = np.argmax(q[s])\n",
    "    A[best_action] = 1.0\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#learning funtion for the montecarlo on policy control\n",
    "# given an episode = [(state, action, reward)] update the Q function\n",
    "def on_policy_control(env, num_episodes, discount_factor = 1.0, epsilon = 0.1, returns_sum = defaultdict(float), returns_count = defaultdict(float), Q = defaultdict(lambda: np.zeros(env.action_space.n))):\n",
    "    for i_episode in range(1,num_episodes + 1):\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(50):\n",
    "            probs = epsilon_greedy_policy(state, env.action_space.n, Q, epsilon)\n",
    "            action = np.random.choice(np.arange(len(probs)), p = probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        sa_in_episode = set((x[0], x[1]) for x in episode)\n",
    "        for state, action in sa_in_episode:\n",
    "            sa_pair = (state, action)\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode) if x[0] == state and x[1] == action)\n",
    "            G = sum([x[2] for x in episode[first_occurence_idx:]])\n",
    "            returns_sum[sa_pair] += G\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "    return Q, returns_sum, returns_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learning funtion for the montecarlo off policy control\n",
    "# given an episode = [(state, action, reward)] update the Q function\n",
    "def off_policy_control(env, num_episodes, discount_factor = 1, Q = defaultdict(lambda: np.zeros(env.action_space.n)), C = defaultdict(lambda: np.zeros(env.action_space.n))):\n",
    "    for i_episode in range(1,num_episodes + 1):\n",
    "        print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "\n",
    "        while True:\n",
    "            probs = random_policy(state, env.action_space.n)\n",
    "            action = np.random.choice(np.arange(len(probs)), p = probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        for t in range(len(episode))[::-1]:\n",
    "            state, action, reward = episode[t]\n",
    "            G = discount_factor * G + reward\n",
    "            C[state][action] += W\n",
    "            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
    "\n",
    "            if action != np.argmax(greedy_policy(state)):\n",
    "                break\n",
    "            W = W * 1./greedy_policy(state)[action]\n",
    "    return Q, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-23 00:43:32,982] Making new env: FrozenLake-v0\n",
      "[2017-01-23 00:43:32,993] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-23 00:43:32,995] Clearing 6 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50000/50000."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env_name = 'FrozenLake-v0'\n",
    "    env = envs.make(env_name)\n",
    "    outdir = \"/Users/jacopo/openaigym/project/MC/results/\" + env_name\n",
    "    env = wrappers.Monitor(env, outdir, force=True)\n",
    "    env.seed(0)\n",
    "    Q, ret_sum, ret_count = on_policy_control(env.unwrapped, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50000/50000."
     ]
    }
   ],
   "source": [
    "    Q, ret_sum, ret_count = on_policy_control(env.unwrapped, 50000, ret_sum, ret_count, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n"
     ]
    }
   ],
   "source": [
    "    state = env.unwrapped.reset()\n",
    "    env.unwrapped.render()\n",
    "\n",
    "    while True:\n",
    "        probs = epsilon_greedy_policy(state, env.unwrapped.action_space.n, Q, 0.1)\n",
    "        action = np.random.choice(np.arange(len(probs)), p = probs)\n",
    "        next_state, reward, done, _ = env.unwrapped.step(action)\n",
    "        env.unwrapped.render()\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
